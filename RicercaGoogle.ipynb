{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def google_search(keywords, sites):\n",
        "    \"\"\"\n",
        "    Esegue una ricerca su Google per ciascun sito in base alle parole chiave specificate.\n",
        "\n",
        "    Args:\n",
        "      keywords: Una lista di parole chiave.\n",
        "      sites: Una lista di siti su cui effettuare la ricerca.\n",
        "\n",
        "    Returns:\n",
        "      Un dizionario contenente i risultati della ricerca per ogni sito.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for site in sites:\n",
        "        # Crea una richiesta HTTP a Google.\n",
        "        url = f\"https://www.google.com/search?q=site:{site} {'+'.join(keywords)}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Verifica se la richiesta ha avuto successo.\n",
        "        if response.status_code == 200:\n",
        "            # Estrai il codice HTML della risposta.\n",
        "            html = response.text\n",
        "\n",
        "            # Crea un oggetto BeautifulSoup per il codice HTML.\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            # Estrai i link dai risultati della ricerca.\n",
        "            links = [link.get('href') for link in soup.find_all(\"a\", href=True) if \"url?q=\" in link['href']]\n",
        "\n",
        "            # Filtra i link pertinenti e funzionanti.\n",
        "            cleaned_links = []\n",
        "            for link in links:\n",
        "                cleaned_link = link.split(\"url?q=\")[1].split(\"&sa=\")[0]\n",
        "                if site in cleaned_link and is_valid_url(cleaned_link):\n",
        "                    cleaned_links.append(cleaned_link)\n",
        "\n",
        "            results[site] = cleaned_links\n",
        "\n",
        "        else:\n",
        "            print(f\"Errore durante la richiesta a Google per il sito {site}.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def is_valid_url(url):\n",
        "    \"\"\"\n",
        "    Verifica se un URL è valido.\n",
        "\n",
        "    Args:\n",
        "      url: URL da verificare.\n",
        "\n",
        "    Returns:\n",
        "      True se l'URL è valido, False altrimenti.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True)\n",
        "        return response.status_code == 200\n",
        "    except requests.RequestException:\n",
        "        return False\n",
        "\n",
        "def analyze_site_content(url, keywords):\n",
        "    \"\"\"\n",
        "    Analizza il contenuto di una pagina web alla ricerca delle keywords specificate.\n",
        "\n",
        "    Args:\n",
        "      url: URL della pagina web da analizzare.\n",
        "      keywords: Una lista di keywords da cercare nel contenuto della pagina.\n",
        "\n",
        "    Returns:\n",
        "      True se tutte le keywords sono trovate nel contenuto della pagina, False altrimenti.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            text = soup.get_text().lower()\n",
        "            for keyword in keywords:\n",
        "                if keyword.lower() not in text:\n",
        "                    return False\n",
        "            return True\n",
        "        return False\n",
        "    except requests.RequestException:\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Interfaccia utente per inserire i siti web e le parole chiave.\n",
        "    sites = input(\"Inserisci i siti web su cui effettuare la ricerca (separati da virgola): \").split(\",\")\n",
        "    keywords = input(\"Inserisci le parole chiave (separate da virgola): \").split(\",\")\n",
        "\n",
        "    # Esegue la ricerca per ogni sito.\n",
        "    results = google_search(keywords, sites)\n",
        "\n",
        "    if results:\n",
        "        # Analizza il contenuto dei siti trovati per le keywords specificate.\n",
        "        for site, links in results.items():\n",
        "            print(f\"Risultati per {site}:\")\n",
        "            for link in links:\n",
        "                if analyze_site_content(link, keywords):\n",
        "                    print(f\"{link}\")\n",
        "    else:\n",
        "        print(\"Nessun risultato trovato.\")\n"
      ],
      "metadata": {
        "id": "PQ9vqR5P40tr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}